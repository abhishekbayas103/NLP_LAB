{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCmyxWYsTEnS",
        "outputId": "1372bcd1-4ade-453e-bbdc-0328a62e2814"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import (WhitespaceTokenizer,\n",
        "                           WordPunctTokenizer,\n",
        "                           TreebankWordTokenizer,\n",
        "                           TweetTokenizer,\n",
        "                           MWETokenizer)\n",
        "\n",
        "text =\"The user's password isn't secure; we'll need to update it.\"\n",
        "tk_white = WhitespaceTokenizer()\n",
        "print(\"Whitespace:\", tk_white.tokenize(text))\n",
        "tk_punct = WordPunctTokenizer()\n",
        "print(\"Punctuation:\", tk_punct.tokenize(text))\n",
        "tk_tree = TreebankWordTokenizer()\n",
        "print(\"Treebank:  \", tk_tree.tokenize(text))\n",
        "tk_tweet = TweetTokenizer()\n",
        "print(\"Tweet:     \", tk_tweet.tokenize(text))\n",
        "tk_mwe = MWETokenizer([('Natural', 'Language')])\n",
        "mwe_text = \"I love Natural Language Processing\"\n",
        "tokens = mwe_text.split()\n",
        "print(\"MWE:       \", tk_mwe.tokenize(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aqpZPHpTYFv",
        "outputId": "8d7203fd-6fe7-4949-f792-baf75a51badb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace: ['The', \"user's\", 'password', \"isn't\", 'secure;', \"we'll\", 'need', 'to', 'update', 'it.']\n",
            "Punctuation: ['The', 'user', \"'\", 's', 'password', 'isn', \"'\", 't', 'secure', ';', 'we', \"'\", 'll', 'need', 'to', 'update', 'it', '.']\n",
            "Treebank:   ['The', 'user', \"'s\", 'password', 'is', \"n't\", 'secure', ';', 'we', \"'ll\", 'need', 'to', 'update', 'it', '.']\n",
            "Tweet:      ['The', \"user's\", 'password', \"isn't\", 'secure', ';', \"we'll\", 'need', 'to', 'update', 'it', '.']\n",
            "MWE:        ['I', 'love', 'Natural_Language', 'Processing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "words = [\"running\", \"generous\", \"universities\", \"easily\"]\n",
        "porter = PorterStemmer()\n",
        "print(\"Porter:  \", [porter.stem(w) for w in words])\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "print(\"Snowball:\", [snowball.stem(w) for w in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuC-lo3KThAF",
        "outputId": "e271218a-ff77-406f-834c-baf388325653"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter:   ['run', 'gener', 'univers', 'easili']\n",
            "Snowball: ['run', 'generous', 'univers', 'easili']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(\"Lemma (default):\", lemmatizer.lemmatize(\"running\"))\n",
        "print(\"Lemma (verb):   \", lemmatizer.lemmatize(\"running\", pos='v'))\n",
        "print(\"Stemmed 'better':\", porter.stem(\"better\"))\n",
        "print(\"Lemmatized 'better':\", lemmatizer.lemmatize(\"better\", pos='a'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhN1swmbTopk",
        "outputId": "cacb029e-da02-42c5-fba7-b839af56ddaa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemma (default): running\n",
            "Lemma (verb):    run\n",
            "Stemmed 'better': better\n",
            "Lemmatized 'better': good\n"
          ]
        }
      ]
    }
  ]
}